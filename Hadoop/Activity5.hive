hive> show tables;
OK
files
word_counts
Time taken: 2.378 seconds, Fetched: 2 row(s)
hive> show databases;
OK
default
Time taken: 0.025 seconds, Fetched: 1 row(s)
hive> create database office;
OK
Time taken: 0.637 seconds
hive> show databases;
OK
default
office
Time taken: 0.034 seconds, Fetched: 2 row(s)
hive> use office;
OK
Time taken: 0.034 seconds
hive> show tables;
OK
Time taken: 0.038 seconds
hive> CREATE TABLE employee
    > (id INT, name STRING, dept STRING, yoj INT, salary INT)
    > ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
    > TBLPROPERTIES ("skip.header.line.count"="1");
OK
Time taken: 4.15 seconds
hive> DESCRIBE employee;
OK
id                      int
name                    string
dept                    string
yoj                     int
salary                  int
Time taken: 2.348 seconds, Fetched: 5 row(s)
hive> select count(*) from employee;
OK
0
Time taken: 7.524 seconds, Fetched: 1 row(s)
hive> LOAD DATA LOCAL INPATH '/root/EmpData.csv' INTO TABLE employee;
Loading data to table office.employee
OK
Time taken: 7.272 seconds
hive> select count(*) from employee;
Query ID = root_20221122171951_c50ee195-35a3-42d1-ae09-28e7b1ae1f63
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1669135567576_0001, Tracking URL = http://f5350e5fcdfc:8088/proxy/application_1669135567576_0001/
Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1669135567576_0001
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2022-11-22 17:23:40,516 Stage-1 map = 0%,  reduce = 0%
2022-11-22 17:24:41,040 Stage-1 map = 0%,  reduce = 0%
2022-11-22 17:25:41,049 Stage-1 map = 0%,  reduce = 0%
2022-11-22 17:26:20,697 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 74.32 sec
2022-11-22 17:26:50,309 Stage-1 map = 100%,  reduce = 67%, Cumulative CPU 78.73 sec
2022-11-22 17:27:07,942 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 89.81 sec
MapReduce Total cumulative CPU time: 1 minutes 29 seconds 880 msec
Ended Job = job_1669135567576_0001
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 89.88 sec   HDFS Read: 13143 HDFS Write: 102 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 29 seconds 880 msec
OK
15
Time taken: 456.478 seconds, Fetched: 1 row(s)
hive> INSERT OVERWRITE DIRECTORY '/user/root/output'
    > ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
    > SELECT * FROM employee;
Query ID = root_20221122173858_e82dd65c-cfaa-49d0-99f6-a7dc4507974b
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1669135567576_0002, Tracking URL = http://f5350e5fcdfc:8088/proxy/application_1669135567576_0002/
Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1669135567576_0002
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2022-11-22 17:39:19,657 Stage-1 map = 0%,  reduce = 0%
2022-11-22 17:39:42,357 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 23.76 sec
MapReduce Total cumulative CPU time: 24 seconds 240 msec
Ended Job = job_1669135567576_0002
Stage-3 is selected by condition resolver.
Stage-2 is filtered out by condition resolver.
Stage-4 is filtered out by condition resolver.
Moving data to directory hdfs://f5350e5fcdfc:9000/user/root/output/.hive-staging_hive_2022-11-22_17-38-58_827_4107507139543321649-1/-ext-10000
Moving data to directory /user/root/output
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1   Cumulative CPU: 24.24 sec   HDFS Read: 5634 HDFS Write: 480 SUCCESS
Total MapReduce CPU Time Spent: 24 seconds 240 msec
OK
Time taken: 50.689 seconds
hive> dfs -ls /user/root/output
    > ;
Found 1 items
-rw-r--r--   1 root supergroup        480 2022-11-22 17:39 /user/root/output/000000_0
hive> dfs -cat /user/root/output/000000_0
    > ;
1,Ian,Quality Assurance,2021,28113
2,Beatrice,Tech Support,2021,35330
3,Vladimir,Human Resources,2020,51445
4,Whitney,IT,2020,23818
5,Leslie,Customer Service,2021,59882
6,Bernard,IT,2021,50330
7,Mary,Customer Service,2021,26558
8,Jerome,RnD,2021,45333
9,Joshua,IT,2021,59538
10,Keane,Human Resources,2022,46500
11,Velma,Human Resources,2022,19816
12,Rogan,Tech Support,2022,27554
13,Aurelia,RnD,2021,20762
14,Merrill,Quality Assurance,2021,59660
15,Blaine,Tech Support,2022,28768