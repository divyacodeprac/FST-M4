hive> CREATE TABLE episodeV ( name STRING , line STRING)
    > ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
    > TBLPROPERTIES ("skip.header.line.count"="2");
OK
Time taken: 5.878 seconds
hive> show tables;
OK
episodeiv
episodev
files
word_counts
Time taken: 0.281 seconds, Fetched: 4 row(s)
hive> LOAD DATA LOCAL INPATH '/root/episode5.txt' INTO TABLE episodeV;
Loading data to table default.episodev
OK
Time taken: 5.199 seconds
hive> SELECT name,COUNT(name) AS no_of_lines from episodeV GROUP BY name ORDER BY no_of_lines;
Query ID = root_20221122183556_9cdf0c4a-d2cd-462a-be1a-45166043a1b4
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1669135567576_0007, Tracking URL = http://f5350e5fcdfc:8088/proxy/application_1669135567576_0007/
Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1669135567576_0007
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2022-11-22 18:36:22,701 Stage-1 map = 0%,  reduce = 0%
2022-11-22 18:36:50,005 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 19.19 sec
2022-11-22 18:37:34,950 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 28.27 sec
MapReduce Total cumulative CPU time: 28 seconds 270 msec
Ended Job = job_1669135567576_0007
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1669135567576_0008, Tracking URL = http://f5350e5fcdfc:8088/proxy/application_1669135567576_0008/
Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1669135567576_0008
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2022-11-22 18:38:11,881 Stage-2 map = 0%,  reduce = 0%
2022-11-22 18:38:23,445 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 5.8 sec
2022-11-22 18:39:10,378 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 17.83 sec
MapReduce Total cumulative CPU time: 17 seconds 830 msec
Ended Job = job_1669135567576_0008
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 28.27 sec   HDFS Read: 61975 HDFS Write: 1530 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 17.83 sec   HDFS Read: 9046 HDFS Write: 1333 SUCCESS
Total MapReduce CPU Time Spent: 46 seconds 100 msec
OK
FIRST CONTROLLER        1
ASSISTANT OFFICER       1
WOMAN CONTROLLER        1
CAPTAIN 1
STRANGE VOICE   1
SECOND THREEPIO 1
SECOND OFFICER  1
SECOND CONTROLLER       1
REBEL FIGHTER   1
REBEL CAPTAIN   1
PILOTS  1
PILOT   1
OFFICER 1
MAN'S VOICE     1
IMPERIAL SOLDIER        1
HOBBIE  1
HEAD CONTROLLER 1
LIEUTENANT      2
TRACKING OFFICER        2
IMPERIAL OFFICER        2
MEDICAL DROID   2
SENIOR CONTROLLER       2
COMMUNICATIONS OFFICER  2
INTERCOM VOICE  2
DERLIN  3
TRENCH OFFICER  3
ANNOUNCER       3
CONTROLLER      3
BEN'S VOICE     4
DACK    4
JANSON  4
BOBA FETT       4
OZZEL   5
EMPEROR 5
NEEDA   5
ZEV     6
DECK OFFICER    7
VEERS   7
WEDGE   8
BEN     11
RIEEKAN 13
CREATURE        21
PIETT   23
YODA    36
VADER   56
LANDO   61
THREEPIO        92
LEIA    114
LUKE    128
HAN     182
Time taken: 202.316 seconds, Fetched: 50 row(s)
hive> INSERT OVERWRITE DIRECTORY '/user/root/outputs/export' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
    > SELECT name,COUNT(name) AS no_of_lines from episodeV GROUP BY name ORDER BY no_of_lines;
Query ID = root_20221122184133_4ef83f9d-07dd-46ce-be41-526ab97e8433
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1669135567576_0009, Tracking URL = http://f5350e5fcdfc:8088/proxy/application_1669135567576_0009/
Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1669135567576_0009
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2022-11-22 18:42:04,500 Stage-1 map = 0%,  reduce = 0%
2022-11-22 18:43:05,010 Stage-1 map = 0%,  reduce = 0%, Cumulative CPU 12.76 sec
2022-11-22 18:43:06,871 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 20.35 sec
2022-11-22 18:43:44,859 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 29.49 sec
MapReduce Total cumulative CPU time: 29 seconds 640 msec
Ended Job = job_1669135567576_0009
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1669135567576_0010, Tracking URL = http://f5350e5fcdfc:8088/proxy/application_1669135567576_0010/
Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1669135567576_0010
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2022-11-22 18:44:40,878 Stage-2 map = 0%,  reduce = 0%
2022-11-22 18:44:52,889 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 3.97 sec
2022-11-22 18:45:18,988 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 10.6 sec
MapReduce Total cumulative CPU time: 10 seconds 600 msec
Ended Job = job_1669135567576_0010
Moving data to directory /user/root/outputs/export
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 29.64 sec   HDFS Read: 62086 HDFS Write: 1530 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 10.6 sec   HDFS Read: 8640 HDFS Write: 646 SUCCESS
Total MapReduce CPU Time Spent: 40 seconds 240 msec
OK
Time taken: 238.365 seconds
hive> INSERT OVERWRITE LOCAL DIRECTORY '/user/root/outputs/export' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
    > SELECT name,COUNT(name) AS no_of_lines from episodeV GROUP BY name ORDER BY no_of_lines;
Query ID = root_20221122184556_e449d254-5f0f-471d-8a82-47cb746a60c4
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1669135567576_0011, Tracking URL = http://f5350e5fcdfc:8088/proxy/application_1669135567576_0011/
Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1669135567576_0011
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2022-11-22 18:46:34,367 Stage-1 map = 0%,  reduce = 0%
2022-11-22 18:47:02,383 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 35.64 sec
2022-11-22 18:47:32,469 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 51.09 sec
MapReduce Total cumulative CPU time: 51 seconds 90 msec
Ended Job = job_1669135567576_0011
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1669135567576_0012, Tracking URL = http://f5350e5fcdfc:8088/proxy/application_1669135567576_0012/
Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1669135567576_0012
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2022-11-22 18:48:17,048 Stage-2 map = 0%,  reduce = 0%
2022-11-22 18:48:39,628 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 3.81 sec
2022-11-22 18:48:52,537 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 9.43 sec
MapReduce Total cumulative CPU time: 9 seconds 430 msec
Ended Job = job_1669135567576_0012
Moving data to local directory /user/root/outputs/export
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 51.09 sec   HDFS Read: 62086 HDFS Write: 1530 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 9.43 sec   HDFS Read: 8662 HDFS Write: 646 SUCCESS
Total MapReduce CPU Time Spent: 1 minutes 0 seconds 520 msec
OK
Time taken: 186.832 seconds